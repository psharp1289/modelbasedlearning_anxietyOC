{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gillan Model\n",
    "\n",
    "\n",
    "$Q_{MF0_{t}}$ = TD(0) Value of action 1 when reaching 2nd stage at time t\n",
    "\n",
    "$Q_{MF1_{t}}$ = TD(1) MF Value of action 1 when seeing reward after 2nd stage choice\n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition counter is updated. For example if state1, action1 led to state 2 once, and on the next transition, the same transition occurs, the counting matrix would be updated as follows:\n",
    "\n",
    "$T_{counting}=\\begin{bmatrix}\n",
    "1+1 & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$T$ can be one of two matrices at and given trial \n",
    "$T_{1} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}$ or $T_{2}=\\begin{bmatrix}\n",
    "0.3 & 0.7 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}$ at any given trial. \n",
    "\n",
    "This is determined by the $T_{counting}$ matrix. When $T_{counting}(1,1) + T_{counting}(2,2) > T_{counting}(1,2) + T_{counting}(2,1)$, then $T_{1}$ is used.\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF0{t+1}}=Q_{MF0_{t}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t}})$\n",
    "\n",
    "$Q_{MF1_{t+1}}=Q_{MF1_{t}} + \\alpha_{2}(R-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T \\cdot Q_{MF2_{t}})$\n",
    "\n",
    "Each Q value has its own beta in the following softmax:\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Note this includes a perseveration parameter that has a beta on the last action taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gillan + TL Model\n",
    "\n",
    "$Q_{MF0_{t}}$ = TD(0) Value of action 1 when reaching 2nd stage at time t\n",
    "\n",
    "$Q_{MF1_{t}}$ = TD(1) MF Value of action 1 when seeing reward after 2nd stage choice\n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\gamma$ = learning rate for state transitions\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition estimate is updated with a learning rate, and probabilities are at that time normalized. For instance, if action 1 is taken and transition to state 2: \n",
    "\n",
    "$P(s_1,a_1,s_2)= P(s_1,a_1,s_2) + \\gamma(1-P(s_1,a_1,s_2))$\n",
    "\n",
    "and\n",
    "\n",
    "$P(s_1,a_1,s_3)= 1-P(s_1,a_1,s_2)$\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF0{t+1}}=Q_{MF0_{t}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t}})$\n",
    "\n",
    "$Q_{MF1_{t+1}}=Q_{MF1_{t}} + \\alpha_{2}(R-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T \\cdot Q_{MF2_{t}})$\n",
    "\n",
    "Each Q value has its own beta in the following softmax:\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Note this includes a perseveration parameter that has a beta on the last action taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed TL Model\n",
    "\n",
    "$Q_{MF0_{t}}$ = TD(0) Value of action 1 when reaching 2nd stage at time t\n",
    "\n",
    "$Q_{MF1_{t}}$ = TD(1) MF Value of action 1 when seeing reward after 2nd stage choice\n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\gamma$ = FIXED (fitted hierarchically to full sample)\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition estimate is updated with a learning rate, and probabilities are at that time normalized. For instance, if action 1 is taken and transition to state 2: \n",
    "\n",
    "$P(s_1,a_1,s_2)= P(s_1,a_1,s_2) + \\gamma(1-P(s_1,a_1,s_2))$\n",
    "\n",
    "and\n",
    "\n",
    "$P(s_1,a_1,s_3)= 1-P(s_1,a_1,s_2)$\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF0{t+1}}=Q_{MF0_{t}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t}})$\n",
    "\n",
    "$Q_{MF1_{t+1}}=Q_{MF1_{t}} + \\alpha_{2}(R-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T \\cdot Q_{MF2_{t}})$\n",
    "\n",
    "Each Q value has its own beta in the following softmax:\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Note this includes a perseveration parameter that has a beta on the last action taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes model\n",
    "\n",
    "\n",
    "$Q_{MF0_{t}}$ = TD(0) Value of action 1 when reaching 2nd stage at time t\n",
    "\n",
    "$Q_{MF1_{t}}$ = TD(1) MF Value of action 1 when seeing reward after 2nd stage choice\n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition counter is updated. For example if state1, action1 led to state 2 once, and on the next transition, the same transition occurs, the counting matrix would be updated as follows:\n",
    "\n",
    "$T_{counting}=\\begin{bmatrix}\n",
    "1+1 & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$T$ can be one of two matrices at and given trial \n",
    "$T_{1} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}$ or $T_{2}=\\begin{bmatrix}\n",
    "0.3 & 0.7 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}$ at any given trial. \n",
    "\n",
    "The first column of the matrix is action 1, which can transition to state 2 or 3 (rows 1 and 2 respectively).\n",
    "\n",
    "Categorical prior ($p$ is a free parameter) on transition matrices = $[p \\,\\,\\,\\,\\,(1-p)]$ \n",
    "\n",
    "Henceforth $p$ is $p_1$ and $1-p$ is $p_2$. Each refers to the probability that either of the two matrices delineated above is the correct transition matrix.\n",
    "\n",
    "Probabilities are updated according to Bayes rule:\n",
    "\n",
    "$p_1 = Bernoulli(all,0.7,hits)(p_1)$\n",
    "\n",
    "$p_2=Bernoulli(all,0.3,hits)(p_2)$\n",
    "        \n",
    "$p_{total}=p_1+p_2$\n",
    "\n",
    "$p_{1}=\\frac{p_{1}}{p_{total}}$\n",
    "\n",
    "$p_{2}=\\frac{p_{2}}{p_{total}}$\n",
    "\n",
    "Here \"all\" refers the running sum of experienced transitions. \n",
    "\n",
    "\"Hits\" refers to evidence in favor of matrix $T_{1}$, which are defined as the common transitions. For example, hits comprise experiencing state 2 from action 1 (entry (1,1) in the matrix) or state 3 from action 2 (entry (2,2) in the matrix). \n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF0{t+1}}=Q_{MF0_{t}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t}})$\n",
    "\n",
    "$Q_{MF1_{t+1}}=Q_{MF1_{t}} + \\alpha_{2}(R-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T_1 \\cdot Q_{MF2_{t}})(p_1)+argmax(T_2 \\cdot Q_{MF2_{t}})(p_2)$\n",
    "\n",
    "Note the MB action value is a weighted combination of MB values according to each possible transition matrix weighted by their current posterior probability\n",
    "\n",
    "Each Q value has its own beta in the following softmax:\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Note this includes a perseveration parameter that has a beta on the last action taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MB only model\n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\gamma$ = learning rate for state transitions\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition estimate is updated with a learning rate, and probabilities are at that time normalized. For instance, if action 1 is taken and transition to state 2: \n",
    "\n",
    "$P(s_1,a_1,s_2)_{t+1}= P(s_1,a_1,s_2)_{t} + \\gamma(1-P(s_1,a_1,s_2)_{t})$\n",
    "\n",
    "and\n",
    "\n",
    "$P(s_1,a_1,s_3)_{t+1}= 1-P(s_1,a_1,s_2)_{t+1}$\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T \\cdot Q_{MF2_{t}})$\n",
    "\n",
    "Each Q value has its own beta in the following softmax:\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{(\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Note this includes a perseveration parameter that has a beta on the last action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daw Model\n",
    "\n",
    "$Q_{MF1_{t}}$ = MF Value of action 1 \n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$\\lambda$ = eligibility trace\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition counter is updated. For example if state1, action1 led to state 2 once, and on the next transition, the same transition occurs, the counting matrix would be updated as follows:\n",
    "\n",
    "$T_{counting}=\\begin{bmatrix}\n",
    "1+1 & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$T$ can be one of two matrices at and given trial \n",
    "$T_{1} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}$ or $T_{2}=\\begin{bmatrix}\n",
    "0.3 & 0.7 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}$ at any given trial. \n",
    "\n",
    "This is determined by the $T_{counting}$ matrix. When $T_{counting}(1,1) + T_{counting}(2,2) > T_{counting}(1,2) + T_{counting}(2,1)$, then $T_{1}$ is used.\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF1{t+1}}=Q_{MF1_{t}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF1_{t+1}}=Q_{MF1_{t}} + \\lambda\\alpha_{2}(R-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T \\cdot Q_{MF2_{t}})$\n",
    "\n",
    "Q values for 1-stage actions are integrated in the following way:\n",
    "\n",
    "$Q_{integrated} = w(Q_{MB})+(1-w)(Q_{MF1})$\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{\\beta[Q_{integrated}+\\rho(M)]}$\n",
    "\n",
    "where $\\rho$ is a perseveration parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daw + TL model\n",
    "\n",
    "$Q_{MF1_{t}}$ = MF Value of action 1 \n",
    "\n",
    "$Q_{MF2_{t}}$ = TD Value of action 2\n",
    "\n",
    "$Q_{MB_{t}}$ = Model-based value of action 1\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$\n",
    "\n",
    "$\\lambda$ = eligibility trace\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition estimate is updated with a learning rate, and probabilities are at that time normalized. For instance, if action 1 is taken and transition to state 2: \n",
    "\n",
    "$P(s_1,a_1,s_2)= P(s_1,a_1,s_2) + \\gamma(1-P(s_1,a_1,s_2))$\n",
    "\n",
    "and\n",
    "\n",
    "$P(s_1,a_1,s_3)= 1-P(s_1,a_1,s_2)$\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "$Q_{MF1{t+1}}=Q_{MF1_{t}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF1_{t+1}}=Q_{MF1_{t}} + \\lambda\\alpha_{2}(R-Q_{MF1_{t}})$\n",
    "\n",
    "$Q_{MF2_{t+1}}=Q_{MF2_{t}} + \\alpha(R-Q_{MF2_{t}})$\n",
    "\n",
    "$Q_{MB_{t+1}} = argmax(T \\cdot Q_{MF2_{t}})$\n",
    "\n",
    "Q values for 1-stage actions are integrated in the following way:\n",
    "\n",
    "$Q_{integrated} = w(Q_{MB})+(1-w)(Q_{MF1})$\n",
    "\n",
    "$P(a_{1},s_{1}) \\propto e^{\\beta[Q_{integrated}+\\rho(M)]}$\n",
    "\n",
    "where $\\rho$ is a perseveration parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
