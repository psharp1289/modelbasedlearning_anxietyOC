{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV + Count\n",
    "\n",
    "- The following models including \"2AV\" in their model names are slight variations of the SARSA algorithm for model-free learning (Rummery and Niranjan, 1994). We changed first-action learning to include two model-free q-values representing action-value updates. The first action-value represents the prediction of which second-stage one will arrive in, each of which has its own value depending on how rewarded it has been in the recent past. The second first-stage action value represents the prediction that the first-stage action will be rewarded after the second-stage choice. Indeed, this less traditional model of the two-step task was used in Gillan et al. (2016). Separating these first-stage action values in turn removes the requirement for an eligibility trace. All models use the Bellman equation to derive model-based action values.\n",
    "\n",
    "### Variables\n",
    "\n",
    "Below, t=time, s=state, a=action. At stage one, two images appeared, one of which could be selected with a given action. The image was always chosen with a certain action that did not change across the task. Each action led to two possible states (determined by a transition matrix), wherein each state had two unique images. At this second stage, an image again is selected with a given action. Note only an \"s\" subscript is used for second-stage action values, since one could either transition to state 2 or 3. The selection of this image would lead to a monetary reward determined by a latent probability that drifted across the task (see Gillan et al., 2016). \n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$ Transition matrix\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "- First stage: \n",
    "\n",
    "$Q_{MF0_{t,a}}$ = First-stage action value predicting value at second stage.\n",
    "\n",
    "$Q_{MF1_{t,a}}$ = First-stage action value predicting reward after second stage.\n",
    "\n",
    "$Q_{MB_{t,a}}$ = Model-based value of action 1\n",
    "\n",
    "- Second stage:\n",
    "\n",
    "$Q_{MF2_{t,s,a}}$ = Second-stage action value predicting reward.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Free Parameters\n",
    "\n",
    "All alpha parameters below were drawn from Beta priors that spans the 0-1 range. \n",
    "\n",
    "$\\alpha_{1}$ = learning rate for both updates on first-stage action values. \n",
    "\n",
    "$\\alpha_{2}$ = learning rate for for update on second action value. \n",
    "\n",
    "All beta parameters below were drawn from Gamma priors that spans all positive real numbers.\n",
    "\n",
    "$\\beta_{MF0}$ = inverse temperature for $Q_{MF0}$ at first stage. \n",
    "\n",
    "$\\beta_{MF1}$ = inverse temperature for $Q_{MF1}$ at first stage. \n",
    "\n",
    "$\\beta_{MB}$ = inverse temperature for $Q_{MB}$ at first stage.\n",
    "\n",
    "$\\beta_{st}$ = strength of perseveration at first stage. This multiplies the M vector, which the previously enacted first-stage action. \n",
    "\n",
    "$\\beta_{MF2}$ = inverse temperature for $Q_{MF2}$ at second stage.\n",
    "\n",
    "### Learning computations\n",
    "\n",
    "- Updating the transition matrix: \n",
    "\n",
    "Each trial, a transition counter is updated. For example if state1, action1 led to state 2 once, and on the next transition, the same transition occurs, the counting matrix would be updated as follows:\n",
    "\n",
    "$T_{counting}=\\begin{bmatrix}\n",
    "1+1 & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$T$ can be one of two matrices at and given trial \n",
    "$T_{1} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}$ or $T_{2}=\\begin{bmatrix}\n",
    "0.3 & 0.7 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}$ at any given trial. \n",
    "\n",
    "This is determined by the $T_{counting}$ matrix. When $T_{counting}(1,1) + T_{counting}(2,2) > T_{counting}(1,2) + T_{counting}(2,1)$, then $T_{1}$ is used.\n",
    "\n",
    "- Updating action-values\n",
    "\n",
    "$Q_{MF0_{t+1,a}}=Q_{MF0_{t,a}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t,a}})$\n",
    "\n",
    "$Q_{MF1_{t+1,a}}=Q_{MF1_{t,a}} + \\alpha_{1}(R-Q_{MF1_{t,a}})$\n",
    "\n",
    "$Q_{MF2_{t+1,s,a}}=Q_{MF2_{t,s,a}} + \\alpha_{2}(R-Q_{MF2_{t,s,a}})$\n",
    "\n",
    "Model-based q-values are then computed via the Bellman equation:\n",
    "\n",
    "$Q_{MB_{t+1}} = P(s_{2}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{2},a_{i})+P(s_{3}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{3},a_{i})$.\n",
    "\n",
    "### Decision computations\n",
    "First-stage action: \n",
    "\n",
    "$P(a,s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Secon-stage action:\n",
    "\n",
    "$P(a,s_{2}) \\propto e^{(\\beta_{MF2}Q_{MF2})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV + LR\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$ transition matrix \n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "- First stage: \n",
    "\n",
    "$Q_{MF0_{t,a}}$ = First-stage action value predicting value at second stage.\n",
    "\n",
    "$Q_{MF1_{t,a}}$ = First-stage action value predicting reward after second stage.\n",
    "\n",
    "$Q_{MB_{t,a}}$ = Model-based value of action 1\n",
    "\n",
    "- Second stage:\n",
    "\n",
    "$Q_{MF2_{t,s,a}}$ = Second-stage action value predicting reward.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Free Parameters\n",
    "\n",
    "All alpha parameters below were drawn from Beta priors that spans the 0-1 range. \n",
    "\n",
    "$\\alpha_{1}$ = learning rate for both updates on first-stage action values. \n",
    "\n",
    "$\\alpha_{2}$ = learning rate for for update on second action value. \n",
    "\n",
    "$\\gamma$ = learning rate for state transitions\n",
    "\n",
    "All beta parameters below were drawn from Gamma priors that spans all positive real numbers.\n",
    "\n",
    "$\\beta_{MF0}$ = inverse temperature for $Q_{MF0}$ at first stage. \n",
    "\n",
    "$\\beta_{MF1}$ = inverse temperature for $Q_{MF1}$ at first stage. \n",
    "\n",
    "$\\beta_{MB}$ = inverse temperature for $Q_{MB}$ at first stage.\n",
    "\n",
    "$\\beta_{st}$ = strength of perseveration at first stage. This multiplies the M vector, which the previously enacted first-stage action.\n",
    "\n",
    "$\\beta_{MF2}$ = inverse temperature for $Q_{MF2}$ at second stage.\n",
    "\n",
    "### Learning computations\n",
    "\n",
    "- Updating the transition matrix: \n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition estimate is updated with a learning rate, and probabilities are at that time normalized. For instance, if action 1 is taken and transition to state 2: \n",
    "\n",
    "$P(s_1,a_1,s_2)_{t+1}= P(s_1,a_1,s_2)_{t} + \\gamma(1-P(s_1,a_1,s_2)_{t})$\n",
    "\n",
    "and\n",
    "\n",
    "$P(s_1,a_1,s_3)_{t+1}= 1-P(s_1,a_1,s_2)_{t+1}$\n",
    "\n",
    "- Updating action-values\n",
    "\n",
    "$Q_{MF0_{t+1,a}}=Q_{MF0_{t,a}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t,a}})$\n",
    "\n",
    "$Q_{MF1_{t+1,a}}=Q_{MF1_{t,a}} + \\alpha_{1}(R-Q_{MF1_{t,a}})$\n",
    "\n",
    "$Q_{MF2_{t+1,s,a}}=Q_{MF2_{t,s,a}} + \\alpha_{2}(R-Q_{MF2_{t,s,a}})$\n",
    "\n",
    "Model-based q-values are then computed via the Bellman equation:\n",
    "\n",
    "$Q_{MB_{t+1}} = P(s_{2}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{2},a_{i})+P(s_{3}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{3},a_{i})$.\n",
    "\n",
    "### Decision computations\n",
    "First-stage action: \n",
    "\n",
    "$P(a,s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Secon-stage action:\n",
    "\n",
    "$P(a,s_{2}) \\propto e^{(\\beta_{MF2}Q_{MF2})}$\n",
    "\n",
    "\n",
    "$\\alpha_{1}$ = learning rate for $Q_{MF0}$\n",
    "\n",
    "$\\alpha_{2}$ = learning rate for $Q_{MF1}$ and $Q_{MB}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV LR + Counterfactual\n",
    "\n",
    "Same as 2AV + LR except that transitions for actions not taken are updated as if the not-taken action led to the state than was not experienced for the taken action. This counterfactual inference is predicated on assumption (that was told to participants and experienced in practice) that the two actions cannot lead most often to the same state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV + Dynamic LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as 2AV + LR except here the $\\gamma$ decays to 0 on each trial by the following equation:\n",
    "\n",
    "$\\gamma_{t}= \\frac{1}{\\epsilon+\\text{N}_{action}}$\n",
    "\n",
    "where $\\epsilon$ determine the starting learning rate, and $\\text{N}_{action}$ is a tally of how many times a given action was taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV + Dynamic LR + Intercept\n",
    "\n",
    "Same as 2AV + Dynamic LR except here the $\\gamma$ decays to a variable baseline LR, $\\omega$:\n",
    "\n",
    "$\\gamma_{t}= \\omega+\\frac{1-\\omega}{\\epsilon+\\text{N}_{action}}$\n",
    "\n",
    "where $\\epsilon$ determines time it will take to decay to baselin $\\omega$ learning rate, and $\\text{N}_{action}$ is a tally of how many times a given action was taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV + Fixed LR\n",
    "\n",
    "Same as 2AV + LR except here,  $\\gamma$ is fixed across subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2AV + Bayes\n",
    "$R$ = reward\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$ transition matrix \n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "- First stage: \n",
    "\n",
    "$Q_{MF0_{t,a}}$ = First-stage action value predicting value at second stage.\n",
    "\n",
    "$Q_{MF1_{t,a}}$ = First-stage action value predicting reward after second stage.\n",
    "\n",
    "$Q_{MB_{t,a}}$ = Model-based value of action 1\n",
    "\n",
    "- Second stage:\n",
    "\n",
    "$Q_{MF2_{t,s,a}}$ = Second-stage action value predicting reward.\n",
    "\n",
    "- Transition matrices:\n",
    "\n",
    "$p_{1}$ represents the belief that the true transition matrix is\n",
    "$T_{1} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "Whereas $p_{2}$ represents the belief that the true transition matrix is $T_{2}=\\begin{bmatrix}\n",
    "0.3 & 0.7 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}$ at any given trial.\n",
    "\n",
    "### Fixed parameter\n",
    "\n",
    "The beta prior defining evidence in favor of Transition Matrix 1 was initialized with mode=0.5\n",
    "\n",
    "### Free Parameters\n",
    "\n",
    "All alpha parameters below were drawn from Beta priors that spans the 0-1 range. \n",
    "\n",
    "$\\alpha_{1}$ = learning rate for both updates on first-stage action values. \n",
    "\n",
    "$\\alpha_{2}$ = learning rate for for update on second action value. \n",
    "\n",
    "$\\kappa$ = concentration of prior over belief in either possible transition matrix.\n",
    "\n",
    "All beta parameters below were drawn from Gamma priors that spans all positive real numbers.\n",
    "\n",
    "$\\beta_{MF0}$ = inverse temperature for $Q_{MF0}$ at first stage. \n",
    "\n",
    "$\\beta_{MF1}$ = inverse temperature for $Q_{MF1}$ at first stage. \n",
    "\n",
    "$\\beta_{MB}$ = inverse temperature for $Q_{MB}$ at first stage.\n",
    "\n",
    "$\\beta_{st}$ = strength of perseveration at first stage. This multiplies the M vector, which the previously enacted first-stage action.\n",
    "\n",
    "$\\beta_{MF2}$ = inverse temperature for $Q_{MF2}$ at second stage.\n",
    "\n",
    "### Learning computations\n",
    "\n",
    "- Updating the transition matrix:\n",
    "\n",
    "The mode (fixed) and concentration (free) of the beta distribution defining the prior belief in T1 and T2 was converted to E1 (evidence in favor of T1) and E2 (evidence in favor of T2) parameters describing the shape of the beta distribution by the following equations:\n",
    "\n",
    "$E1 = \\text{mode}(\\kappa-2)+1$.\n",
    "\n",
    "$E2 = \\text{(1-mode)}(\\kappa-2)+1$.\n",
    "\n",
    "The posterior of the beta prior is updated analytically:\n",
    "\n",
    "$E1=E1+1$ when common transitions predicted by T1 are experienced.\n",
    "\n",
    "and\n",
    "\n",
    "$E2=E2+1$ when common transitions predicted by T2 are experienced.\n",
    "\n",
    "Each time model-based action values are computed, evidence for each transition matrix is derived from the mean of the beta distribution by:\n",
    "\n",
    "$p_{1} = \\frac{E1}{E1+E2}$ which represents the probability that T1 is the true transition matrix.\n",
    "\n",
    "$p_{2} = 1-p_{1}$.\n",
    "\n",
    "$Q_{MB_{t+1}} = (\\text{Bellman Equation for} \\,\\, T_{1})(p_{1})+(\\text{Bellman Equation for}\\,\\, T_{2})(p_{2})$.\n",
    "\n",
    "- Updating action-values\n",
    "\n",
    "$Q_{MF0_{t+1,a}}=Q_{MF0_{t,a}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t,a}})$\n",
    "\n",
    "$Q_{MF1_{t+1,a}}=Q_{MF1_{t,a}} + \\alpha_{1}(R-Q_{MF1_{t,a}})$\n",
    "\n",
    "$Q_{MF2_{t+1,s,a}}=Q_{MF2_{t,s,a}} + \\alpha_{2}(R-Q_{MF2_{t,s,a}})$\n",
    "\n",
    "Model-based q-values are then computed via the Bellman equation:\n",
    "\n",
    "$Q_{MB_{t+1}} = P(s_{2}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{2},a_{i})+P(s_{3}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{3},a_{i})$.\n",
    "\n",
    "### Decision computations\n",
    "First-stage action: \n",
    "\n",
    "$P(a,s_{1}) \\propto e^{(\\beta_{MF0}Q_{MF0}+\\beta_{MF1}Q_{MF1}+\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Secon-stage action:\n",
    "\n",
    "$P(a,s_{2}) \\propto e^{(\\beta_{MF2}Q_{MF2})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MB\n",
    "\n",
    "Here, first-stage actions are only influences by model-based planning and a perseveration parameter.\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$ transition matrix \n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "- First stage: \n",
    "\n",
    "$Q_{MF0_{t,a}}$ = First-stage action value predicting value at second stage.\n",
    "\n",
    "$Q_{MF1_{t,a}}$ = First-stage action value predicting reward after second stage.\n",
    "\n",
    "$Q_{MB_{t,a}}$ = Model-based value of action 1\n",
    "\n",
    "- Second stage:\n",
    "\n",
    "$Q_{MF2_{t,s,a}}$ = Second-stage action value predicting reward.\n",
    "\n",
    "\n",
    "### Free Parameters\n",
    "\n",
    "All alpha parameters below were drawn from Beta priors that spans the 0-1 range. \n",
    "\n",
    "$\\alpha_{1}$ = learning rate for both updates on first-stage action values. \n",
    "\n",
    "$\\alpha_{2}$ = learning rate for for update on second action value. \n",
    "\n",
    "$\\gamma$ = learning rate for state transitions\n",
    "\n",
    "All beta parameters below were drawn from Gamma priors that spans all positive real numbers. \n",
    "\n",
    "$\\beta_{MB}$ = inverse temperature for $Q_{MB}$ at first stage.\n",
    "\n",
    "$\\beta_{st}$ = strength of perseveration at first stage. This multiplies the M vector, which retains which action was taken most recently. \n",
    "\n",
    "$\\beta_{MF2}$ = inverse temperature for $Q_{MF2}$ at second stage.\n",
    "\n",
    "### Learning computations\n",
    "\n",
    "- Updating the transition matrix: \n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each trial, a transition estimate is updated with a learning rate, and probabilities are at that time normalized. For instance, if action 1 is taken and transition to state 2: \n",
    "\n",
    "$P(s_1,a_1,s_2)_{t+1}= P(s_1,a_1,s_2)_{t} + \\gamma(1-P(s_1,a_1,s_2)_{t})$\n",
    "\n",
    "and\n",
    "\n",
    "$P(s_1,a_1,s_3)_{t+1}= 1-P(s_1,a_1,s_2)_{t+1}$\n",
    "\n",
    "- Updating action-values\n",
    "\n",
    "$Q_{MF0_{t+1,a}}=Q_{MF0_{t,a}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF0_{t,a}})$\n",
    "\n",
    "$Q_{MF1_{t+1,a}}=Q_{MF1_{t,a}} + \\alpha_{1}(R-Q_{MF1_{t,a}})$\n",
    "\n",
    "$Q_{MF2_{t+1,s,a}}=Q_{MF2_{t,s,a}} + \\alpha_{2}(R-Q_{MF2_{t,s,a}})$\n",
    "\n",
    "Model-based q-values are then computed via the Bellman equation:\n",
    "\n",
    "$Q_{MB_{t+1}} = P(s_{2}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{2},a_{i})+P(s_{3}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{3},a_{i})$.\n",
    "\n",
    "### Decision computations\n",
    "First-stage action: \n",
    "\n",
    "$P(a,s_{1}) \\propto e^{(\\beta_{MB}Q_{MB}+\\beta_{st}M)}$\n",
    "\n",
    "Secon-stage action:\n",
    "\n",
    "$P(a,s_{2}) \\propto e^{(\\beta_{MF2}Q_{MF2})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1AV Model\n",
    "\n",
    "$R$ = reward\n",
    "\n",
    "$T = \\begin{bmatrix}\n",
    "P(s_1,a_1,s_2) & P(s_1,a_2,s_2) \\\\\n",
    "P(s_1,a_1,s_3) & P(s_1,a_2,s_3) \n",
    "\\end{bmatrix}$ Transition matrix\n",
    "\n",
    "- First stage: \n",
    "\n",
    "$Q_{MF1_{t,a}}$ = First-stage action value.\n",
    "\n",
    "$Q_{MB_{t,a}}$ = Model-based value of action 1\n",
    "\n",
    "$M$ = one-hot vector indicating which first-stage action was previously taken.\n",
    "\n",
    "- Second stage:\n",
    "\n",
    "$Q_{MF2_{t,s,a}}$ = Second-stage action value predicting reward.\n",
    "\n",
    "\n",
    "### Free Parameters\n",
    "\n",
    "All parameters below were drawn from Beta priors that spans the 0-1 range. \n",
    "\n",
    "$\\alpha_{1}$ = learning rate for both updates on first-stage action values. \n",
    "\n",
    "$\\alpha_{2}$ = learning rate for for update on second action value.\n",
    "\n",
    "$\\lambda$ = eligibility trace\n",
    "\n",
    "$\\omega$ = weight on model-based control\n",
    "\n",
    "\n",
    "All beta parameters below were drawn from Gamma priors that spans all positive real numbers.\n",
    "\n",
    "$\\beta_{1}$ = inverse temperature for $Q_{MF0}$ at first stage. \n",
    "\n",
    "$\\beta_{2}$ = inverse temperature for $Q_{MB}$ at first stage.\n",
    "\n",
    "$st$ = perseveration parameter\n",
    "\n",
    "\n",
    "### Learning computations\n",
    "\n",
    "- Updating the transition matrix: \n",
    "\n",
    "Each trial, a transition counter is updated. For example if state1, action1 led to state 2 once, and on the next transition, the same transition occurs, the counting matrix would be updated as follows:\n",
    "\n",
    "$T_{counting}=\\begin{bmatrix}\n",
    "1+1 & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$T$ can be one of two matrices at and given trial \n",
    "$T_{1} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}$ or $T_{2}=\\begin{bmatrix}\n",
    "0.3 & 0.7 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}$ at any given trial. \n",
    "\n",
    "This is determined by the $T_{counting}$ matrix. When $T_{counting}(1,1) + T_{counting}(2,2) > T_{counting}(1,2) + T_{counting}(2,1)$, then $T_{1}$ is used.\n",
    "\n",
    "- Updating action-values\n",
    "\n",
    "$Q_{MF1_{t+1,a}}=Q_{MF1_{t,a}} + \\alpha_{1}(Q_{MF2_{t}}-Q_{MF_{t,a}})$\n",
    "\n",
    "$Q_{MF1_{t+2,a}}=Q_{MF1_{t+1,a}} + \\alpha_{1}(R-Q_{MF1_{t+1,a}})$\n",
    "\n",
    "$Q_{MF2_{t+1,s,a}}=Q_{MF2_{t,s,a}} + \\alpha_{2}(R-Q_{MF2_{t,s,a}})$\n",
    "\n",
    "Model-based q-values are then computed via the Bellman equation:\n",
    "\n",
    "$Q_{MB_{t+1}} = P(s_{2}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{2},a_{i})+P(s_{3}|s_{1},a_{i})\\max\\limits_{a\\in \\{ 1,2 \\}}Q_{MF2}(s_{3},a_{i})$.\n",
    "\n",
    "Q values for 1-stage actions are integrated in the following way:\n",
    "\n",
    "$Q_{integrated} = \\omega(Q_{MB})+(1-\\omega)(Q_{MF1})$\n",
    "\n",
    "\n",
    "### Decision computations\n",
    "First-stage action: \n",
    "\n",
    "$P(a,s_{1}) \\propto e^{\\beta_{1}[Q_{integrated}+st(M)]}$\n",
    "\n",
    "Secon-stage action:\n",
    "\n",
    "$P(a,s_{2}) \\propto e^{\\beta_{2}Q_{MF2}}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1AV+ LR model\n",
    "\n",
    "Same as Daw model except state transition estimates are learned in the same way as 2AV + LR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
